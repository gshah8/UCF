{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gshah8/UCF/blob/master/Machine_Learning/HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "KMwRoxS15IAB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# HW 1 \n",
        "\n",
        "This homework will get you up to speed with Python programming, numpy, matplotlib, Keras, gradients, partial derivatives, git, GitHub, Google's colaboratory etc. Have fun!\n",
        "\n",
        "For this homework, you will create neural networks with an input layer and an output layer without any hidden layers. The connections are dense: each input neuron is connected to each output neuron.\n",
        "\n",
        "Instructions for problems 1 and 2:\n",
        "- Load the training and test data using Keras, no validation set needed.\n",
        "- Train 10 classifiers that perform binary classification: *Is the input image the digit i or is it a digit different from i?* Each of the ten classifiers has an input layer consisting of 28 x 28 input neurons and an output layer consisting of a single output neuron.\n",
        "- Implement mini-batch stochastic gradient descent using only numpy, that is, you are not allowed to use TensorFlow/Keras for SGD.\n",
        "- Use ```argmax``` to determine the classifier with the strongest output and declare the corresponding digit as output.\n",
        "\n",
        "## Problem 1\n",
        "Use logistic regression with mean squared error loss."
      ]
    },
    {
      "metadata": {
        "id": "4XwQkWl55lCZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bN-mUe3l9zOH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 2\n",
        "Use logistic regression with binary cross entropy loss."
      ]
    },
    {
      "metadata": {
        "id": "4q9XBuNV99LW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ctzMUL6-MZP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 3\n",
        "- Load the training and test data using Keras, no validation set needed.\n",
        "- Create network with an input layer consisting of 28 x 28 input neurons and an output layer consisting of 10 output neurons.\n",
        "- Use softmax and categorical cross entropy loss.\n",
        "- Implement mini-batch stochastic gradient descent using only numpy, that is, you are not allowed to use TensorFlow/Keras for SGD."
      ]
    },
    {
      "metadata": {
        "id": "6u5AvT1V-PX7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eb20d284-1f04-41e9-bb37-1c034a371da6"
      },
      "cell_type": "code",
      "source": [
        "#Load data\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(train_images_original, train_labels_original), (test_images_original, test_labels_original) = mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-Q_ZrNeDaFvA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for reshaping and normalizing train and test images\n",
        "m = 60000\n",
        "n = 10000\n",
        "image_size = 28*28\n",
        "train_images = train_images_original.reshape((m, image_size))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images_original.reshape((n, image_size))\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GvfhUqR-aR0T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for adding bias to train and test images\n",
        "train_images_b = np.c_[np.ones((m, 1)), train_images]\n",
        "test_images_b = np.c_[np.ones((n, 1)), test_images]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UI3Vz40U6D-y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for adding bias to train and test labels. Doing something like one-hot encoding for labels\n",
        "train_labels = np.zeros((m,10))\n",
        "test_labels = np.zeros((n,10))\n",
        "\n",
        "for i in range(m):\n",
        "  train_labels[i][train_labels_original[i]] = 1\n",
        "  \n",
        "for i in range(n):\n",
        "  test_labels[i][test_labels_original[i]] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uw_DcCKeatQK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for setting weight\n",
        "\n",
        "np.random.seed(42)\n",
        "weight = np.random.randn(image_size+1,10)\n",
        "weight_path_mgd = []\n",
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Owj522VEOwHZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3397
        },
        "outputId": "3c82381e-ae02-4667-983a-8efd08400cd1"
      },
      "cell_type": "code",
      "source": [
        "#this is for linear regression. Change it for logistic regression\n",
        "\n",
        "epochs = 10\n",
        "lr = 0.01\n",
        "\n",
        "weight_path_mgd.append(weight)\n",
        "for epoch in range(epochs):\n",
        "    #shuffled_indices = np.random.permutation(m)\n",
        "    #X_b_shuffled = X_b[shuffled_indices]\n",
        "    #y_shuffled = y[shuffled_indices]\n",
        "    for i in range(0, m, batch_size):\n",
        "      train_images_b_i = train_images_b[i:i+batch_size]\n",
        "      train_labels_b_i = train_labels[i:i+batch_size]\n",
        "      z = train_images_b_i.dot(weight)\n",
        "\n",
        "      #apply activation - softmax\n",
        "      z_exp = np.exp(z)\n",
        "      z_exp_sum = z_exp.sum(axis=1)\n",
        "      a = np.zeros((64,10))\n",
        "      for j in range(len(z)):\n",
        "        a[j] = z_exp[j] / z_exp_sum[j]\n",
        "      \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "      #break\n",
        "    #break\n",
        "    \n",
        "        #a = np.zeros((10,1))\n",
        "        #for j in range(len(z)):\n",
        "        #  a[j] = np.exp(z)\n",
        "\n",
        "#print(z_exp_sum)\n",
        "#print(z[0][0])\n",
        "#print(z_exp[0][0])\n",
        "#print(len(z))\n",
        "#print(a)\n",
        "\n",
        "#sum_arr = np.zeros((64,1))\n",
        "#sum_arr = a.sum(axis=1)\n",
        "#print(sum_arr)\n",
        "\n",
        "\n",
        "        #gradient = 2/batch_size * xi.T.dot(xi.dot(weight) - yi)\n",
        "        #weight = weight - lr * gradient\n",
        "        #weight_path_mgd.append(weight)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[9.99992201e-01 5.30172558e-11 2.41170207e-06 2.14447085e-11\n",
            "  2.39798306e-10 3.70428084e-08 1.41400059e-15 3.01990027e-11\n",
            "  7.91065792e-17 5.35027395e-06]\n",
            " [4.02995514e-04 4.58881522e-02 9.51491574e-01 2.67192153e-07\n",
            "  6.78366206e-11 7.46094154e-09 4.55328231e-10 1.01166581e-10\n",
            "  2.06117659e-08 2.21698239e-03]\n",
            " [2.71613730e-01 9.56631419e-08 5.63826689e-01 6.42513409e-03\n",
            "  7.71115061e-02 7.18447203e-04 1.61696653e-06 5.30817509e-02\n",
            "  8.25035451e-03 1.89706759e-02]\n",
            " [6.22299128e-01 5.37468911e-02 1.38830147e-02 1.76592469e-01\n",
            "  9.88270343e-02 3.22403113e-02 2.13800578e-05 1.19818529e-05\n",
            "  2.32149578e-03 5.62945799e-05]\n",
            " [1.85395283e-01 2.81678790e-03 1.29102175e-05 8.02920092e-01\n",
            "  7.14655929e-10 8.72610938e-03 4.31483160e-09 1.01341914e-04\n",
            "  6.29428334e-09 2.74635511e-05]\n",
            " [1.72445802e-05 6.33220959e-04 1.68088807e-01 5.21259437e-04\n",
            "  8.82969252e-02 1.08164312e-03 4.62139927e-06 1.58683774e-01\n",
            "  4.39510399e-16 5.82672504e-01]\n",
            " [6.76709756e-04 1.72366859e-08 2.04207203e-05 9.99202460e-01\n",
            "  3.08086687e-06 9.50553187e-09 9.72495201e-05 4.96910786e-11\n",
            "  1.20802838e-11 5.21379303e-08]\n",
            " [9.99999853e-01 6.67724174e-12 1.55921753e-11 1.74197954e-10\n",
            "  9.62304817e-12 1.85240848e-08 3.50983802e-17 1.28474098e-07\n",
            "  1.69476596e-18 1.47815945e-10]\n",
            " [1.78940349e-01 5.71069194e-05 2.73037412e-01 5.05332041e-01\n",
            "  1.84537253e-06 5.80976124e-05 4.15966217e-02 7.45094224e-08\n",
            "  9.65134235e-07 9.75486139e-04]\n",
            " [4.38860718e-05 6.16823038e-11 8.59341228e-01 1.35452004e-01\n",
            "  5.09316879e-10 5.15738230e-03 2.74275578e-12 3.81374313e-06\n",
            "  4.16765761e-08 1.64341355e-06]\n",
            " [9.91728238e-01 1.55035405e-06 5.46711300e-03 4.00158086e-08\n",
            "  2.01670624e-05 1.21009947e-07 2.78261524e-03 6.14737362e-10\n",
            "  3.02431858e-12 1.54362198e-07]\n",
            " [6.07468272e-07 9.38270574e-01 5.54765475e-02 1.65508776e-06\n",
            "  8.97873354e-08 8.02306553e-04 5.38887823e-03 9.21190440e-06\n",
            "  5.76383922e-10 5.01288947e-05]\n",
            " [1.26291409e-01 1.91033525e-05 1.13508362e-01 1.77095630e-05\n",
            "  2.53073818e-09 1.45259184e-08 7.59743645e-01 1.64572157e-05\n",
            "  1.50446646e-08 4.03282232e-04]\n",
            " [1.01425940e-04 1.89191904e-09 9.97114732e-01 3.90600442e-04\n",
            "  4.33696183e-04 1.91074531e-03 2.96490449e-06 4.50688562e-07\n",
            "  1.49998112e-09 4.53811802e-05]\n",
            " [5.15879985e-06 3.06951581e-08 9.99570923e-01 1.57443853e-06\n",
            "  2.11829485e-11 1.06627005e-07 4.07298552e-04 7.03185657e-13\n",
            "  8.31838615e-09 1.48999115e-05]\n",
            " [3.99137667e-03 2.28540527e-02 1.15654065e-03 8.68103793e-01\n",
            "  2.92927280e-09 1.03734778e-01 1.88300549e-07 1.12000928e-10\n",
            "  6.36170027e-07 1.58631570e-04]\n",
            " [1.45824616e-05 1.22701189e-05 9.99972612e-01 8.64323149e-08\n",
            "  3.02863540e-12 5.97646330e-11 4.34856864e-07 1.18484654e-08\n",
            "  2.18973835e-15 1.82164396e-09]\n",
            " [2.16015583e-01 1.32710071e-07 7.79811104e-01 2.63354958e-05\n",
            "  9.98945729e-10 4.13062556e-03 1.10344025e-10 5.11219726e-11\n",
            "  1.28959060e-11 1.62177519e-05]\n",
            " [1.87004855e-02 2.49807808e-09 3.71419627e-02 8.58962939e-01\n",
            "  3.36027718e-05 2.95828070e-03 1.80853774e-06 1.51850067e-02\n",
            "  6.68923352e-02 1.23576109e-04]\n",
            " [5.65341895e-03 1.90900912e-02 1.14466963e-02 8.84331550e-01\n",
            "  1.36785390e-05 2.32105480e-02 5.62395592e-02 7.65223050e-07\n",
            "  3.79786449e-08 1.36543877e-05]\n",
            " [9.37662239e-01 1.81714756e-09 7.29909443e-09 3.45959284e-03\n",
            "  3.78635714e-06 5.70878262e-02 3.48545530e-13 8.33903580e-04\n",
            "  6.69110466e-05 8.85732272e-04]\n",
            " [1.42821884e-03 9.77566698e-01 2.05653093e-02 3.07435104e-04\n",
            "  3.15084878e-15 3.83052625e-08 9.20095882e-10 1.48573446e-13\n",
            "  1.69252050e-09 1.32297538e-04]\n",
            " [1.04852856e-06 3.70982312e-04 4.44392799e-01 5.53593495e-01\n",
            "  1.37178867e-04 1.11822091e-04 9.87976279e-04 3.96753881e-04\n",
            "  4.63479868e-07 7.48029132e-06]\n",
            " [2.59753294e-01 1.18344066e-01 1.88928415e-04 5.05354785e-01\n",
            "  8.97887358e-02 2.63402185e-02 1.21202696e-05 1.12462518e-06\n",
            "  2.09997788e-04 6.73003519e-06]\n",
            " [6.09252065e-02 1.80895218e-04 4.71547062e-01 2.29932678e-03\n",
            "  4.64804646e-01 8.04201384e-07 2.41970567e-04 1.40438873e-08\n",
            "  6.27150468e-11 7.51677111e-08]\n",
            " [9.99966422e-01 3.27249859e-05 5.81143702e-07 3.31103938e-08\n",
            "  9.92105139e-18 1.42551366e-14 5.32321583e-11 2.38965278e-07\n",
            "  1.85253121e-19 2.20906525e-11]\n",
            " [2.90929300e-04 9.58167572e-01 1.07107766e-02 6.49349209e-04\n",
            "  2.95578888e-07 3.00450322e-02 1.01273690e-04 4.81363007e-06\n",
            "  3.72850348e-09 2.99543044e-05]\n",
            " [1.00000000e+00 8.60753170e-12 2.39488506e-12 1.59197831e-12\n",
            "  1.04804862e-12 6.79034813e-11 8.27764319e-15 2.97726513e-14\n",
            "  1.36874510e-21 3.81600846e-12]\n",
            " [2.67617914e-01 5.25337657e-02 7.05038042e-03 3.09920523e-10\n",
            "  6.47191795e-10 6.72788353e-01 8.47483441e-09 3.46205309e-15\n",
            "  4.68581325e-13 9.57751726e-06]\n",
            " [6.61486477e-07 1.23819760e-05 1.62738184e-02 9.82585257e-01\n",
            "  1.25986867e-04 1.18007181e-04 6.20365607e-05 5.91708237e-11\n",
            "  1.41706760e-05 8.07679816e-04]\n",
            " [7.41876470e-01 1.46726733e-06 1.29088415e-05 8.91682014e-03\n",
            "  4.72486049e-05 2.59869050e-04 1.31996658e-04 1.59294061e-08\n",
            "  9.82990453e-07 2.48752220e-01]\n",
            " [3.02498998e-01 4.34550207e-06 2.60849071e-01 3.37423596e-03\n",
            "  1.29242328e-09 4.32976609e-01 4.14969331e-05 1.11288207e-07\n",
            "  6.49447954e-09 2.55123942e-04]\n",
            " [3.72204590e-03 2.71658049e-10 9.93047517e-01 8.43679689e-10\n",
            "  1.41437097e-08 1.45180177e-08 2.13815475e-08 3.23001387e-03\n",
            "  5.31145803e-08 3.19291868e-07]\n",
            " [9.68777519e-02 5.70418067e-02 6.04599533e-04 8.02000071e-01\n",
            "  1.56613237e-04 4.32943568e-02 4.61482472e-06 1.85279488e-09\n",
            "  6.47338330e-08 2.01197474e-05]\n",
            " [9.61545761e-01 5.05810076e-14 3.84541946e-02 6.66441609e-12\n",
            "  2.52701734e-09 2.26041829e-08 2.57370886e-11 1.80176911e-08\n",
            "  8.97597344e-10 8.25861127e-13]\n",
            " [1.15236135e-02 7.32084204e-03 4.65743600e-03 1.82624602e-03\n",
            "  4.23238460e-03 9.57751847e-01 2.63564675e-05 1.26213889e-02\n",
            "  1.31271803e-05 2.67586726e-05]\n",
            " [9.99808685e-01 6.14782562e-17 1.91313356e-04 3.28056520e-12\n",
            "  6.30116015e-10 1.09616201e-11 4.32589016e-16 4.23684632e-12\n",
            "  7.01693803e-16 1.14787808e-09]\n",
            " [9.87800585e-01 7.25381235e-05 1.20662183e-02 1.13685789e-08\n",
            "  7.74111532e-11 6.86496086e-14 3.57365370e-08 6.06053275e-05\n",
            "  5.79217450e-09 2.21750735e-10]\n",
            " [1.07874842e-03 2.24014639e-01 3.09859960e-04 4.86373250e-01\n",
            "  4.09643415e-02 2.13317057e-01 3.43443256e-09 4.68724871e-10\n",
            "  4.04101468e-07 3.39416957e-02]\n",
            " [1.37291783e-04 2.78934566e-13 9.99184793e-01 1.15884643e-08\n",
            "  7.12992750e-11 4.61404941e-06 6.18828585e-10 6.73287900e-04\n",
            "  7.05750356e-14 9.93115882e-10]\n",
            " [1.65202326e-05 2.04535898e-07 1.23751801e-04 3.62535308e-05\n",
            "  2.19666749e-08 5.37711279e-04 9.99285472e-01 1.33463802e-13\n",
            "  5.28850115e-10 6.45182560e-08]\n",
            " [9.98965778e-01 8.37530903e-11 3.32736146e-05 1.03572358e-05\n",
            "  8.28995793e-04 1.61589807e-04 8.70865969e-15 3.00818374e-12\n",
            "  1.81886330e-14 5.16422950e-09]\n",
            " [5.48311622e-09 1.80655012e-04 8.62632232e-07 8.71074611e-04\n",
            "  1.68052884e-08 9.98630929e-01 1.60847700e-05 1.76889513e-12\n",
            "  7.76765815e-08 3.00294154e-04]\n",
            " [1.40758775e-05 1.62951047e-05 9.43404382e-01 1.25890080e-02\n",
            "  3.69419906e-06 2.18962483e-02 2.31379160e-05 7.50259409e-10\n",
            "  6.22185972e-11 2.20531580e-02]\n",
            " [4.55379754e-01 5.05086639e-06 1.18039290e-04 1.51160499e-01\n",
            "  8.86274755e-03 3.83444176e-01 4.39717897e-07 1.75846997e-04\n",
            "  6.98667890e-09 8.53439142e-04]\n",
            " [1.04000455e-02 6.68890552e-09 6.96784415e-04 3.10602458e-04\n",
            "  1.65461436e-08 9.88586064e-01 3.18073312e-09 2.53166445e-08\n",
            "  8.06281016e-11 6.45179083e-06]\n",
            " [1.84180918e-01 2.93083327e-06 5.95013737e-07 7.80819834e-01\n",
            "  3.40666473e-02 9.23183269e-04 2.46679780e-07 3.00826663e-06\n",
            "  1.48463886e-12 2.63653122e-06]\n",
            " [8.12761413e-06 5.46609097e-06 9.99983985e-01 4.82655862e-09\n",
            "  8.68826851e-15 2.41456371e-06 1.97166080e-09 2.29893531e-10\n",
            "  1.04559857e-12 9.86143548e-12]\n",
            " [1.23732284e-02 3.95402504e-04 4.67505165e-04 9.86636434e-01\n",
            "  1.03867746e-05 3.31167910e-06 2.62072013e-05 6.78851318e-05\n",
            "  1.95049733e-09 1.96371286e-05]\n",
            " [9.99824736e-01 7.33943915e-08 3.12262714e-06 1.79253178e-05\n",
            "  1.20349492e-04 2.51916795e-06 2.84932821e-10 3.36642243e-09\n",
            "  5.57464420e-12 3.12699815e-05]\n",
            " [7.59382533e-03 1.37450873e-06 7.15291457e-04 1.39498396e-05\n",
            "  6.67567128e-06 1.60607989e-07 9.91656337e-01 1.23819949e-05\n",
            "  3.47131518e-09 7.04620216e-11]\n",
            " [8.26854507e-01 4.98282064e-02 1.00845494e-03 2.69417882e-03\n",
            "  4.64126676e-13 5.84572179e-03 7.25771649e-12 1.82446478e-07\n",
            "  3.29193551e-15 1.13768748e-01]\n",
            " [9.97924475e-01 9.37303203e-04 1.12997655e-07 3.23610206e-09\n",
            "  9.01460187e-11 1.74942738e-08 3.11102364e-11 5.13154249e-17\n",
            "  3.54227773e-09 1.13808396e-03]\n",
            " [2.09611325e-03 5.48785665e-01 3.77271938e-01 5.13178077e-02\n",
            "  1.52873533e-04 9.04007494e-06 1.14452859e-02 2.63390697e-07\n",
            "  1.15536482e-03 7.76564856e-03]\n",
            " [3.18088753e-04 2.65677329e-09 3.10230613e-09 1.78374101e-09\n",
            "  7.07319149e-11 9.99667911e-01 1.15383667e-05 8.81646607e-12\n",
            "  6.86258648e-13 2.45409739e-06]\n",
            " [5.55679792e-02 5.75572445e-02 2.79171474e-01 2.31989260e-01\n",
            "  3.22008280e-03 3.65353242e-01 1.45286150e-06 2.48748080e-11\n",
            "  2.34833478e-11 7.13926482e-03]\n",
            " [3.21314356e-09 9.99999981e-01 1.43699510e-10 1.46338737e-09\n",
            "  7.74846602e-22 3.58961866e-13 6.62058143e-16 1.17796359e-14\n",
            "  1.07623511e-14 1.44489188e-08]\n",
            " [6.50696017e-03 3.11334700e-04 9.91714466e-01 9.05801335e-04\n",
            "  1.85982283e-08 6.72850467e-05 1.05215214e-05 7.11316022e-08\n",
            "  4.26203324e-04 5.73384792e-05]\n",
            " [6.14137404e-02 2.30312936e-11 7.46909607e-14 9.38556520e-01\n",
            "  6.02016150e-14 3.01209297e-10 7.54520294e-18 2.97395930e-05\n",
            "  9.83924747e-13 6.47725213e-12]\n",
            " [1.58719252e-02 1.84090247e-01 5.21173181e-03 7.83894222e-01\n",
            "  1.05768736e-02 2.18263725e-04 3.26788144e-05 2.12846087e-06\n",
            "  2.89171893e-05 7.30122911e-05]\n",
            " [1.87661660e-01 4.46090343e-02 3.27495213e-03 1.16067015e-08\n",
            "  2.44275600e-12 4.46782292e-05 2.95798351e-13 3.59723993e-05\n",
            "  9.33727272e-02 6.71000964e-01]\n",
            " [9.59938363e-01 1.79567248e-08 1.04761886e-05 2.06115455e-02\n",
            "  1.90466864e-02 3.81560728e-04 4.38608678e-13 1.20642900e-06\n",
            "  8.71256860e-06 1.43083517e-06]\n",
            " [2.23948439e-03 6.95856689e-11 9.97527618e-01 1.55614138e-05\n",
            "  6.53102856e-06 7.67614953e-12 7.90075479e-11 2.10234372e-04\n",
            "  5.65342614e-07 5.50418963e-09]\n",
            " [9.99850755e-01 1.18671613e-09 6.03565454e-07 2.28442140e-06\n",
            "  4.07217822e-12 3.10573837e-05 8.76600514e-10 3.09588741e-08\n",
            "  1.21961047e-12 1.15266227e-04]]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "72LAURd9cK0s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Loss "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jM98Wwzi-QJG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 4\n",
        "Reimplement the network from Problem 3 entirely in Keras."
      ]
    },
    {
      "metadata": {
        "id": "g3TYeh_a-S2x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(train_images_original, train_labels_original), (test_images_original, test_labels_original) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W9ZAQlsWL7d_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images = train_images_original.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images_original.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "#train_images_original.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "domURlv2NjFu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(train_labels_original)\n",
        "test_labels = to_categorical(test_labels_original)\n",
        "#test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8johAcgDOBlc",
        "colab_type": "code",
        "outputId": "910b312a-1d03-4c48-8533-49096ad67763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(28*28, activation='relu', input_shape=(28 * 28,)))\n",
        "network.add(layers.Dense(10, activation='softmax'))\n",
        "network.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 784)               615440    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                7850      \n",
            "=================================================================\n",
            "Total params: 623,290\n",
            "Trainable params: 623,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HZT0zpBQOtqM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "network.compile(optimizer='sgd',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lT8y2kRrPaC8",
        "colab_type": "code",
        "outputId": "1be1cfde-8cde-4594-f0e8-7161fa3283ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "history = network.fit(train_images, \n",
        "                      train_labels, \n",
        "                      epochs=epochs, \n",
        "                      batch_size=128, \n",
        "                      validation_data=(test_images, test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 1.0802 - acc: 0.7649 - val_loss: 0.5847 - val_acc: 0.8721\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.5120 - acc: 0.8757 - val_loss: 0.4235 - val_acc: 0.8937\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.4133 - acc: 0.8913 - val_loss: 0.3665 - val_acc: 0.9042\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.3685 - acc: 0.9001 - val_loss: 0.3339 - val_acc: 0.9108\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.3409 - acc: 0.9068 - val_loss: 0.3136 - val_acc: 0.9148\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.3212 - acc: 0.9115 - val_loss: 0.2977 - val_acc: 0.9185\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.3059 - acc: 0.9157 - val_loss: 0.2852 - val_acc: 0.9206\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.2930 - acc: 0.9189 - val_loss: 0.2747 - val_acc: 0.9245\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.2823 - acc: 0.9216 - val_loss: 0.2658 - val_acc: 0.9270\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.2725 - acc: 0.9245 - val_loss: 0.2574 - val_acc: 0.9285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bCzA_kZ-R1GO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "test_loss_values = history_dict['val_loss']\n",
        "epochs_range = range(1, epochs + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f4wF0qIOR5HF",
        "colab_type": "code",
        "outputId": "a8ee6441-dbc7-4e38-d2c0-1373742a4cbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(epochs_range, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs_range, test_loss_values, 'ro', label='Test loss')\n",
        "plt.title('Training and test loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4rEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "r9ZsMF6iVBmv",
        "colab_type": "code",
        "outputId": "59ba03f8-3319-4151-c5c8-44dfe1f7bec5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 65us/step\n",
            "Test accuracy: 0.9285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cHSfWEWJ-TpS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 5\n",
        "Extend the network from Problem 4 by adding new features.\n",
        "- Round the grey values of the images to 1 and 0 so you obtain black and white images. Add as an additional feature the number of white regions. For instance, a typical 0 has 2 white regions and 8 has 3. Use the following neighborhoods for pixels:\n",
        "\n",
        "```\n",
        "pixel x,y (indicated by .) is connected to its neighbors (indicated by o):\n",
        "\n",
        " o\n",
        "o.o  \n",
        " o\n",
        " \n",
        "ooo\n",
        "o.o\n",
        "ooo\n",
        "```\n",
        "- Consider the width.\n",
        "- Consider the height.\n",
        "- Come up with other features.\n",
        "\n",
        "You should normalize your new features so that they are at the same scale as the pixel values (between 0 and 1)."
      ]
    },
    {
      "metadata": {
        "id": "IM5yz84l-bAf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}