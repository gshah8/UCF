{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gshah8/UCF/blob/master/Machine_Learning/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5vSpuR2h9REm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# HW 2\n",
        "\n",
        "The goal of this homework is to create a convolutional neural network for the CIFAR10 data set. \n",
        "See [this colab notebook](https://colab.research.google.com/drive/1LZZviWOzvchcXRdZi2IBx3KOpQOzLalf) how to load the CIFAR data in Keras.\n",
        "\n",
        "You should not use any pretrained convnets that come with Keras. You have to create and train your own convnets with Keras from scratch.\n",
        "\n",
        "## Simple hold-out validation\n",
        "\n",
        "Make sure that the data is divided into: \n",
        "\n",
        "- training set (80%)\n",
        "- validation set (20%)\n",
        "- test set. \n",
        "\n",
        "Use the training set to train your neural networks. Evaluate their performance on the validation data set. \n",
        "\n",
        "After trying several different architectures, choose the one that performs\n",
        "best of the validation set. Try at least four different architectures by using data augmentation, using dropout, varying the number of layers, the number of filters, etc.\n",
        "\n",
        "Train this final architecture on the data from the training set and validation set and evaluate its performance on \n",
        "the test set.\n",
        "\n",
        "## k-fold validation\n",
        "\n",
        "Reevaluate your best architecture using k-fold validation with k=5, that is, the size of the validation fold is 20%. Does the accuracy/loss obtain by k-fold validation differ from the accuracy/loss obtain by simple hold-out validation."
      ]
    },
    {
      "metadata": {
        "id": "tGt5GSXu_g9t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the CIFAR10 data set\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GaciVEq0_dts",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "36341544-e914-4e24-93c2-de4a5185f395"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import to_categorical\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 20s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-AYEf3umAHQm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Explore the format of the CIFAR10 data set"
      ]
    },
    {
      "metadata": {
        "id": "p1hNMiPtAIsN",
        "colab_type": "code",
        "outputId": "74d4751c-eb58-42a0-a368-5ee7467fced2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_images.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "J6KJNpLlAtLb",
        "colab_type": "code",
        "outputId": "73b304fe-35d4-4992-94f9-3230c256425d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_images.ndim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "8_SDVSLrAx1L",
        "colab_type": "code",
        "outputId": "bb6ec80b-5894-44aa-a97f-04e3815cffa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_labels.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "14iRg_dJA0Iv",
        "colab_type": "code",
        "outputId": "36fdacd7-c8fc-4c1d-c3c2-f1d9368a87b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_labels.ndim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "Mqbp1flDTvGx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Simple Hold Validation for the models below"
      ]
    },
    {
      "metadata": {
        "id": "ghUwL-nmUZx8",
        "colab_type": "code",
        "outputId": "db4e62c4-105d-4fdb-dd6e-54cd6eb5623b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#divide training data into training and validation data\n",
        "rand_idx = np.random.permutation(len(train_images))\n",
        "val_idx = rand_idx[0:10000]\n",
        "train_idx = rand_idx[10000:]\n",
        "\n",
        "train_images_new, train_labels_new = train_images[train_idx] , train_labels[train_idx]\n",
        "val_images, val_labels = train_images[val_idx] , train_labels[val_idx]\n",
        "\n",
        "print(train_labels_new[0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FbOE5y_-EzOV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Normalize train, validation and test data\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "w4u6Vq_KE6Oe",
        "colab_type": "code",
        "outputId": "09bd677c-fc08-424d-9901-b1d96f530a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#normalize\n",
        "train_images_norm = (train_images_new/255).astype('float32')\n",
        "val_images_norm = (val_images/255).astype('float32')\n",
        "test_images_norm = (test_images/255).astype('float32')\n",
        "\n",
        "#one-hot encoding\n",
        "train_labels_norm = to_categorical(train_labels_new)\n",
        "val_labels_norm = to_categorical(val_labels)\n",
        "test_labels_norm = to_categorical(test_labels)\n",
        "\n",
        "print(train_labels_norm[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9vynlXErFcqO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Build a basic model (without regularization and data augmentation)"
      ]
    },
    {
      "metadata": {
        "id": "4d7o1oJLFeqm",
        "colab_type": "code",
        "outputId": "b981b1a1-f648-4f70-fd3b-4525327e877c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "# set up the layers\n",
        "\n",
        "model = models.Sequential()\n",
        "#conv layers   \n",
        "#1\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding = 'same', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#2\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding = 'same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#3\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding = 'same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 8, 8, 64)          36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                65600     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 122,570\n",
            "Trainable params: 122,570\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UY6Yl69XH4b4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "             loss='categorical_crossentropy',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ULo_pCDVIJd_",
        "colab_type": "code",
        "outputId": "e2abe6df-5a6e-485a-b9b0-2ff6a8607afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "history = model.fit(train_images_norm, \n",
        "                      train_labels_norm, \n",
        "                      epochs=epochs, \n",
        "                      batch_size=64, \n",
        "                      validation_data=(val_images_norm, val_labels_norm))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "40000/40000 [==============================] - 13s 316us/step - loss: 1.5815 - acc: 0.4276 - val_loss: 1.3115 - val_acc: 0.5238\n",
            "Epoch 2/20\n",
            "40000/40000 [==============================] - 8s 195us/step - loss: 1.1898 - acc: 0.5784 - val_loss: 1.0927 - val_acc: 0.6217\n",
            "Epoch 3/20\n",
            "40000/40000 [==============================] - 8s 196us/step - loss: 1.0071 - acc: 0.6487 - val_loss: 1.0335 - val_acc: 0.6364\n",
            "Epoch 4/20\n",
            "40000/40000 [==============================] - 8s 197us/step - loss: 0.9048 - acc: 0.6846 - val_loss: 0.9386 - val_acc: 0.6726\n",
            "Epoch 5/20\n",
            "40000/40000 [==============================] - 8s 195us/step - loss: 0.8213 - acc: 0.7162 - val_loss: 0.8847 - val_acc: 0.6927\n",
            "Epoch 6/20\n",
            "40000/40000 [==============================] - 8s 196us/step - loss: 0.7595 - acc: 0.7376 - val_loss: 0.8928 - val_acc: 0.6895\n",
            "Epoch 7/20\n",
            "40000/40000 [==============================] - 8s 197us/step - loss: 0.7037 - acc: 0.7551 - val_loss: 0.8464 - val_acc: 0.7121\n",
            "Epoch 8/20\n",
            "40000/40000 [==============================] - 8s 195us/step - loss: 0.6587 - acc: 0.7690 - val_loss: 0.8349 - val_acc: 0.7165\n",
            "Epoch 9/20\n",
            "40000/40000 [==============================] - 8s 195us/step - loss: 0.6109 - acc: 0.7893 - val_loss: 0.8328 - val_acc: 0.7182\n",
            "Epoch 10/20\n",
            "40000/40000 [==============================] - 8s 195us/step - loss: 0.5675 - acc: 0.8014 - val_loss: 0.9025 - val_acc: 0.7064\n",
            "Epoch 11/20\n",
            "40000/40000 [==============================] - 8s 199us/step - loss: 0.5298 - acc: 0.8167 - val_loss: 0.8964 - val_acc: 0.7080\n",
            "Epoch 12/20\n",
            "40000/40000 [==============================] - 8s 197us/step - loss: 0.4897 - acc: 0.8287 - val_loss: 0.9070 - val_acc: 0.7080\n",
            "Epoch 13/20\n",
            "40000/40000 [==============================] - 8s 197us/step - loss: 0.4580 - acc: 0.8421 - val_loss: 0.8864 - val_acc: 0.7272\n",
            "Epoch 14/20\n",
            "40000/40000 [==============================] - 8s 196us/step - loss: 0.4202 - acc: 0.8521 - val_loss: 0.9358 - val_acc: 0.7172\n",
            "Epoch 15/20\n",
            "40000/40000 [==============================] - 8s 196us/step - loss: 0.3933 - acc: 0.8609 - val_loss: 0.9868 - val_acc: 0.7175\n",
            "Epoch 16/20\n",
            "40000/40000 [==============================] - 8s 195us/step - loss: 0.3600 - acc: 0.8720 - val_loss: 0.9909 - val_acc: 0.7180\n",
            "Epoch 17/20\n",
            "40000/40000 [==============================] - 8s 194us/step - loss: 0.3335 - acc: 0.8821 - val_loss: 1.1051 - val_acc: 0.7084\n",
            "Epoch 18/20\n",
            "40000/40000 [==============================] - 8s 193us/step - loss: 0.3107 - acc: 0.8899 - val_loss: 1.0684 - val_acc: 0.7183\n",
            "Epoch 19/20\n",
            "40000/40000 [==============================] - 8s 194us/step - loss: 0.2836 - acc: 0.9001 - val_loss: 1.1754 - val_acc: 0.7096\n",
            "Epoch 20/20\n",
            "40000/40000 [==============================] - 8s 193us/step - loss: 0.2549 - acc: 0.9093 - val_loss: 1.1712 - val_acc: 0.7149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B-dIySPtN-Gj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can observe that the model is clearly overfitted since the training accuracy is increasing as we increase the number of epochs but the validation accuracy is not increasing.\n",
        "\n",
        "Now, we will try to improve the model by using the four following architectures:\n",
        "1. Add another convolutional layer\n",
        "2. Data Augmentation\n",
        "3. Regularization - Dropout\n",
        "4. Regularization - Batch Normalization\n",
        "5. Strides\n",
        "\n",
        "We will add the above mentioned architectures one by one and observe the training and validation accuracy.\n",
        "In the end, as a final check, we will run the model for test data."
      ]
    },
    {
      "metadata": {
        "id": "kHsI-SikNtfQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Add another convolutional layer"
      ]
    },
    {
      "metadata": {
        "id": "pTnpHLaERnpt",
        "colab_type": "code",
        "outputId": "f5936f02-8525-49c8-8c76-56b7dd773bb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "cell_type": "code",
      "source": [
        "# set up the layers\n",
        "\n",
        "model = models.Sequential()\n",
        "#conv layers   \n",
        "#1\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding = 'same', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#2\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding = 'same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#3\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu',padding = 'same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#4\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding = 'same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 307,786\n",
            "Trainable params: 307,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wEx0Pl9CQlCE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "             loss='categorical_crossentropy',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T7RPGQ7ZQqcV",
        "colab_type": "code",
        "outputId": "52067a42-4532-46b5-c048-20d03aa362f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1394
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 40\n",
        "history = model.fit(train_images_norm, \n",
        "                      train_labels_norm, \n",
        "                      epochs=epochs, \n",
        "                      batch_size=64, \n",
        "                      validation_data=(val_images_norm, val_labels_norm))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "40000/40000 [==============================] - 13s 320us/step - loss: 1.5315 - acc: 0.4386 - val_loss: 1.1855 - val_acc: 0.5765\n",
            "Epoch 2/40\n",
            "40000/40000 [==============================] - 12s 307us/step - loss: 1.0776 - acc: 0.6172 - val_loss: 0.9951 - val_acc: 0.6434\n",
            "Epoch 3/40\n",
            "40000/40000 [==============================] - 12s 307us/step - loss: 0.8594 - acc: 0.6993 - val_loss: 0.8426 - val_acc: 0.7057\n",
            "Epoch 4/40\n",
            "40000/40000 [==============================] - 12s 307us/step - loss: 0.7226 - acc: 0.7481 - val_loss: 0.8352 - val_acc: 0.7143\n",
            "Epoch 5/40\n",
            "40000/40000 [==============================] - 12s 304us/step - loss: 0.6211 - acc: 0.7825 - val_loss: 0.7482 - val_acc: 0.7410\n",
            "Epoch 6/40\n",
            "40000/40000 [==============================] - 12s 302us/step - loss: 0.5231 - acc: 0.8160 - val_loss: 0.7746 - val_acc: 0.7406\n",
            "Epoch 7/40\n",
            "40000/40000 [==============================] - 12s 304us/step - loss: 0.4426 - acc: 0.8442 - val_loss: 0.7628 - val_acc: 0.7428\n",
            "Epoch 8/40\n",
            "40000/40000 [==============================] - 12s 305us/step - loss: 0.3656 - acc: 0.8723 - val_loss: 0.8258 - val_acc: 0.7451\n",
            "Epoch 9/40\n",
            "40000/40000 [==============================] - 12s 303us/step - loss: 0.2990 - acc: 0.8951 - val_loss: 0.8624 - val_acc: 0.7444\n",
            "Epoch 10/40\n",
            "40000/40000 [==============================] - 12s 303us/step - loss: 0.2449 - acc: 0.9131 - val_loss: 0.9339 - val_acc: 0.7481\n",
            "Epoch 11/40\n",
            "40000/40000 [==============================] - 12s 303us/step - loss: 0.2040 - acc: 0.9266 - val_loss: 1.0148 - val_acc: 0.7394\n",
            "Epoch 12/40\n",
            "40000/40000 [==============================] - 12s 300us/step - loss: 0.1731 - acc: 0.9391 - val_loss: 1.1209 - val_acc: 0.7396\n",
            "Epoch 13/40\n",
            "40000/40000 [==============================] - 12s 299us/step - loss: 0.1469 - acc: 0.9477 - val_loss: 1.2422 - val_acc: 0.7324\n",
            "Epoch 14/40\n",
            "40000/40000 [==============================] - 12s 299us/step - loss: 0.1438 - acc: 0.9491 - val_loss: 1.2459 - val_acc: 0.7371\n",
            "Epoch 15/40\n",
            "40000/40000 [==============================] - 12s 300us/step - loss: 0.1322 - acc: 0.9525 - val_loss: 1.2886 - val_acc: 0.7154\n",
            "Epoch 16/40\n",
            "40000/40000 [==============================] - 12s 298us/step - loss: 0.1092 - acc: 0.9620 - val_loss: 1.3680 - val_acc: 0.7348\n",
            "Epoch 17/40\n",
            "40000/40000 [==============================] - 12s 300us/step - loss: 0.1046 - acc: 0.9632 - val_loss: 1.3877 - val_acc: 0.7331\n",
            "Epoch 18/40\n",
            "40000/40000 [==============================] - 12s 299us/step - loss: 0.1127 - acc: 0.9590 - val_loss: 1.4855 - val_acc: 0.7290\n",
            "Epoch 19/40\n",
            "40000/40000 [==============================] - 12s 300us/step - loss: 0.0947 - acc: 0.9671 - val_loss: 1.4615 - val_acc: 0.7248\n",
            "Epoch 20/40\n",
            "40000/40000 [==============================] - 12s 300us/step - loss: 0.1015 - acc: 0.9646 - val_loss: 1.4010 - val_acc: 0.7353\n",
            "Epoch 21/40\n",
            "40000/40000 [==============================] - 12s 300us/step - loss: 0.0773 - acc: 0.9728 - val_loss: 1.7024 - val_acc: 0.7192\n",
            "Epoch 22/40\n",
            "40000/40000 [==============================] - 12s 299us/step - loss: 0.0909 - acc: 0.9683 - val_loss: 1.6964 - val_acc: 0.7246\n",
            "Epoch 23/40\n",
            "40000/40000 [==============================] - 12s 299us/step - loss: 0.0777 - acc: 0.9729 - val_loss: 1.6670 - val_acc: 0.7345\n",
            "Epoch 24/40\n",
            "40000/40000 [==============================] - 12s 298us/step - loss: 0.0904 - acc: 0.9685 - val_loss: 1.6282 - val_acc: 0.7284\n",
            "Epoch 25/40\n",
            "40000/40000 [==============================] - 12s 299us/step - loss: 0.0763 - acc: 0.9734 - val_loss: 1.5943 - val_acc: 0.7314\n",
            "Epoch 26/40\n",
            "40000/40000 [==============================] - 12s 300us/step - loss: 0.0766 - acc: 0.9734 - val_loss: 1.7537 - val_acc: 0.7194\n",
            "Epoch 27/40\n",
            "40000/40000 [==============================] - 12s 300us/step - loss: 0.0767 - acc: 0.9734 - val_loss: 1.7371 - val_acc: 0.7277\n",
            "Epoch 28/40\n",
            "40000/40000 [==============================] - 12s 302us/step - loss: 0.0723 - acc: 0.9756 - val_loss: 1.7052 - val_acc: 0.7240\n",
            "Epoch 29/40\n",
            "40000/40000 [==============================] - 12s 303us/step - loss: 0.0756 - acc: 0.9747 - val_loss: 1.7587 - val_acc: 0.7293\n",
            "Epoch 30/40\n",
            "40000/40000 [==============================] - 12s 305us/step - loss: 0.0718 - acc: 0.9763 - val_loss: 1.8358 - val_acc: 0.7238\n",
            "Epoch 31/40\n",
            "40000/40000 [==============================] - 12s 303us/step - loss: 0.0687 - acc: 0.9763 - val_loss: 1.8430 - val_acc: 0.7308\n",
            "Epoch 32/40\n",
            "40000/40000 [==============================] - 12s 299us/step - loss: 0.0673 - acc: 0.9774 - val_loss: 1.8674 - val_acc: 0.7229\n",
            "Epoch 33/40\n",
            "40000/40000 [==============================] - 12s 299us/step - loss: 0.0662 - acc: 0.9783 - val_loss: 1.7415 - val_acc: 0.7342\n",
            "Epoch 34/40\n",
            "40000/40000 [==============================] - 12s 304us/step - loss: 0.0537 - acc: 0.9820 - val_loss: 1.8849 - val_acc: 0.7314\n",
            "Epoch 35/40\n",
            "40000/40000 [==============================] - 12s 300us/step - loss: 0.0694 - acc: 0.9768 - val_loss: 1.9707 - val_acc: 0.7071\n",
            "Epoch 36/40\n",
            "40000/40000 [==============================] - 12s 305us/step - loss: 0.0641 - acc: 0.9784 - val_loss: 1.8676 - val_acc: 0.7285\n",
            "Epoch 37/40\n",
            "40000/40000 [==============================] - 12s 299us/step - loss: 0.0623 - acc: 0.9792 - val_loss: 1.8125 - val_acc: 0.7341\n",
            "Epoch 38/40\n",
            "40000/40000 [==============================] - 12s 299us/step - loss: 0.0583 - acc: 0.9807 - val_loss: 1.8648 - val_acc: 0.7393\n",
            "Epoch 39/40\n",
            "40000/40000 [==============================] - 12s 299us/step - loss: 0.0610 - acc: 0.9799 - val_loss: 1.9127 - val_acc: 0.7276\n",
            "Epoch 40/40\n",
            "40000/40000 [==============================] - 12s 299us/step - loss: 0.0619 - acc: 0.9796 - val_loss: 1.7303 - val_acc: 0.7341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z0xXI48M5qfA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adding another layer doesn't increase validation accuracy.There is even more overfitting since the training accuracy is around 98% for the 40th epoch whereas it is just around 75% for validation data.\n",
        "\n",
        "Therefore, now we are going to include regularization, data augmentation and dropout one by one to the existing model with 4 layers in order to see how to validation accuracy improves and overfitting is minimized."
      ]
    },
    {
      "metadata": {
        "id": "TNRUlGqDInG5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Add Data Augmentation"
      ]
    },
    {
      "metadata": {
        "id": "qAZpDyf8OKNA",
        "colab_type": "code",
        "outputId": "7e80d24c-41bf-4846-a2c3-daafed8106ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "cell_type": "code",
      "source": [
        "# set up the layers\n",
        "\n",
        "model = models.Sequential()\n",
        "#conv layers   \n",
        "#1\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding = 'same', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#2\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding = 'same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#3\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu',padding = 'same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#4\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding = 'same'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_11 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 307,786\n",
            "Trainable params: 307,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hSaXYN7mb4IX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "             loss='categorical_crossentropy',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eBpx2orPOf1M",
        "colab_type": "code",
        "outputId": "1603b636-870d-4bc5-bf74-370af1644f77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1377
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 40\n",
        "datagen = ImageDataGenerator(\n",
        "    #rescale=1./255,  #our image is already normalized so we dont need this\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "datagen.fit(train_images_norm)\n",
        "\n",
        "history = model.fit_generator(datagen.flow(train_images_norm, train_labels_norm, batch_size=64),\n",
        "                    steps_per_epoch=int(len(train_images_norm) / 64), epochs=epochs,\n",
        "                   verbose=1, validation_data=(val_images_norm, val_labels_norm))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "625/625 [==============================] - 29s 47ms/step - loss: 1.8152 - acc: 0.3275 - val_loss: 1.6358 - val_acc: 0.4227\n",
            "Epoch 2/40\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 1.5297 - acc: 0.4442 - val_loss: 1.2595 - val_acc: 0.5430\n",
            "Epoch 3/40\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 1.3918 - acc: 0.4975 - val_loss: 1.1232 - val_acc: 0.5886\n",
            "Epoch 4/40\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 1.3051 - acc: 0.5313 - val_loss: 1.2593 - val_acc: 0.5589\n",
            "Epoch 5/40\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 1.2511 - acc: 0.5555 - val_loss: 1.0172 - val_acc: 0.6355\n",
            "Epoch 6/40\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 1.1841 - acc: 0.5769 - val_loss: 1.0580 - val_acc: 0.6244\n",
            "Epoch 7/40\n",
            "625/625 [==============================] - 28s 46ms/step - loss: 1.1472 - acc: 0.5904 - val_loss: 0.9873 - val_acc: 0.6492\n",
            "Epoch 8/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 1.1072 - acc: 0.6056 - val_loss: 1.0493 - val_acc: 0.6400\n",
            "Epoch 9/40\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 1.0806 - acc: 0.6189 - val_loss: 0.9751 - val_acc: 0.6644\n",
            "Epoch 10/40\n",
            "625/625 [==============================] - 28s 46ms/step - loss: 1.0541 - acc: 0.6261 - val_loss: 0.9298 - val_acc: 0.6733\n",
            "Epoch 11/40\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 1.0316 - acc: 0.6347 - val_loss: 1.0073 - val_acc: 0.6551\n",
            "Epoch 12/40\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 1.0137 - acc: 0.6406 - val_loss: 0.9301 - val_acc: 0.6798\n",
            "Epoch 13/40\n",
            "625/625 [==============================] - 28s 46ms/step - loss: 0.9881 - acc: 0.6502 - val_loss: 0.9473 - val_acc: 0.6737\n",
            "Epoch 14/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.9720 - acc: 0.6582 - val_loss: 1.0401 - val_acc: 0.6526\n",
            "Epoch 15/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.9546 - acc: 0.6618 - val_loss: 0.9089 - val_acc: 0.6947\n",
            "Epoch 16/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.9466 - acc: 0.6658 - val_loss: 0.8221 - val_acc: 0.7150\n",
            "Epoch 17/40\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 0.9352 - acc: 0.6734 - val_loss: 0.8939 - val_acc: 0.6929\n",
            "Epoch 18/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.9258 - acc: 0.6751 - val_loss: 0.9843 - val_acc: 0.6711\n",
            "Epoch 19/40\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 0.9092 - acc: 0.6815 - val_loss: 0.7847 - val_acc: 0.7263\n",
            "Epoch 20/40\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 0.9032 - acc: 0.6839 - val_loss: 0.9093 - val_acc: 0.6986\n",
            "Epoch 21/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.8807 - acc: 0.6904 - val_loss: 0.7831 - val_acc: 0.7303\n",
            "Epoch 22/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.8781 - acc: 0.6904 - val_loss: 0.8129 - val_acc: 0.7229\n",
            "Epoch 23/40\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 0.8748 - acc: 0.6939 - val_loss: 0.8310 - val_acc: 0.7130\n",
            "Epoch 24/40\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 0.8666 - acc: 0.6987 - val_loss: 0.8288 - val_acc: 0.7195\n",
            "Epoch 25/40\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 0.8534 - acc: 0.6994 - val_loss: 0.8168 - val_acc: 0.7223\n",
            "Epoch 26/40\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 0.8509 - acc: 0.6991 - val_loss: 0.7463 - val_acc: 0.7457\n",
            "Epoch 27/40\n",
            "625/625 [==============================] - 28s 45ms/step - loss: 0.8390 - acc: 0.7049 - val_loss: 0.8055 - val_acc: 0.7254\n",
            "Epoch 28/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.8374 - acc: 0.7077 - val_loss: 0.7688 - val_acc: 0.7401\n",
            "Epoch 29/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.8236 - acc: 0.7101 - val_loss: 0.7787 - val_acc: 0.7366\n",
            "Epoch 30/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.8249 - acc: 0.7110 - val_loss: 0.7800 - val_acc: 0.7399\n",
            "Epoch 31/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.8171 - acc: 0.7125 - val_loss: 0.7262 - val_acc: 0.7481\n",
            "Epoch 32/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.8117 - acc: 0.7130 - val_loss: 0.7923 - val_acc: 0.7324\n",
            "Epoch 33/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.8038 - acc: 0.7195 - val_loss: 0.8223 - val_acc: 0.7255\n",
            "Epoch 34/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.7990 - acc: 0.7226 - val_loss: 0.7708 - val_acc: 0.7391\n",
            "Epoch 35/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.7914 - acc: 0.7224 - val_loss: 0.8241 - val_acc: 0.7188\n",
            "Epoch 36/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.7884 - acc: 0.7255 - val_loss: 0.7646 - val_acc: 0.7425\n",
            "Epoch 37/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.7850 - acc: 0.7245 - val_loss: 0.8299 - val_acc: 0.7263\n",
            "Epoch 38/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.7812 - acc: 0.7245 - val_loss: 0.8688 - val_acc: 0.7147\n",
            "Epoch 39/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.7702 - acc: 0.7321 - val_loss: 0.7917 - val_acc: 0.7334\n",
            "Epoch 40/40\n",
            "625/625 [==============================] - 29s 46ms/step - loss: 0.7693 - acc: 0.7290 - val_loss: 0.7128 - val_acc: 0.7564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U2Nzey546Yh6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Add more Layers, Regularization (kernal_regularizer, batch normalization and dropout) and Strides"
      ]
    },
    {
      "metadata": {
        "id": "1aMBqYWq980h",
        "colab_type": "code",
        "outputId": "5d3c197e-d15f-4328-b95c-34539c0e4d6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "cell_type": "code",
      "source": [
        "#from keras import regularizers\n",
        "# set up the layers\n",
        "weight_decay = 1e-4\n",
        "model = models.Sequential()\n",
        "#conv layers   \n",
        "#1\n",
        "model.add(layers.Conv2D(32, (3, 3), strides = (1,1), activation='relu', padding = 'same', kernel_initializer='he_normal', kernel_regularizer= regularizers.l2(weight_decay), input_shape=(32, 32, 3)))\n",
        "model.add(layers.BatchNormalization())\n",
        "#2\n",
        "model.add(layers.Conv2D(32, (3, 3), strides = (1,1), activation='relu', padding = 'same', kernel_initializer='he_normal', kernel_regularizer= regularizers.l2(weight_decay)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.2))\n",
        "#3\n",
        "model.add(layers.Conv2D(64, (3, 3), strides = (1,1), activation='relu',padding = 'same', kernel_initializer='he_normal', kernel_regularizer= regularizers.l2(weight_decay)))\n",
        "model.add(layers.BatchNormalization())\n",
        "#4\n",
        "model.add(layers.Conv2D(64, (3, 3), strides = (1,1), activation='relu', padding = 'same', kernel_initializer='he_normal', kernel_regularizer= regularizers.l2(weight_decay)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.3))\n",
        "#5\n",
        "model.add(layers.Conv2D(128, (3, 3), strides = (1,1), activation='relu', padding = 'same', kernel_initializer='he_normal', kernel_regularizer= regularizers.l2(weight_decay)))\n",
        "model.add(layers.BatchNormalization())\n",
        "#6\n",
        "model.add(layers.Conv2D(128, (3, 3), strides = (1,1), activation='relu', padding = 'same', kernel_initializer='he_normal', kernel_regularizer= regularizers.l2(weight_decay)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.4))\n",
        "#dense layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 552,362\n",
            "Trainable params: 551,466\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o7voAcZ-_6Mo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "             loss='categorical_crossentropy',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xhzyrOQM_6dC",
        "colab_type": "code",
        "outputId": "4bd10b1c-82d2-4589-dd2b-fc4332bdc809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2115
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 60\n",
        "history = model.fit(train_images_norm, \n",
        "                      train_labels_norm, \n",
        "                      epochs=epochs, \n",
        "                      batch_size=64, \n",
        "                      validation_data=(val_images_norm, val_labels_norm))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/60\n",
            "40000/40000 [==============================] - 34s 855us/step - loss: 1.9566 - acc: 0.3149 - val_loss: 1.7509 - val_acc: 0.4116\n",
            "Epoch 2/60\n",
            "40000/40000 [==============================] - 26s 644us/step - loss: 1.5481 - acc: 0.4656 - val_loss: 1.2440 - val_acc: 0.5953\n",
            "Epoch 3/60\n",
            "40000/40000 [==============================] - 25s 615us/step - loss: 1.3350 - acc: 0.5582 - val_loss: 1.1562 - val_acc: 0.6224\n",
            "Epoch 4/60\n",
            "40000/40000 [==============================] - 24s 611us/step - loss: 1.2023 - acc: 0.6109 - val_loss: 1.0718 - val_acc: 0.6561\n",
            "Epoch 5/60\n",
            "40000/40000 [==============================] - 25s 629us/step - loss: 1.1091 - acc: 0.6485 - val_loss: 0.9570 - val_acc: 0.7043\n",
            "Epoch 6/60\n",
            "40000/40000 [==============================] - 24s 591us/step - loss: 1.0404 - acc: 0.6805 - val_loss: 0.8898 - val_acc: 0.7312\n",
            "Epoch 7/60\n",
            "40000/40000 [==============================] - 23s 587us/step - loss: 0.9790 - acc: 0.7046 - val_loss: 0.8497 - val_acc: 0.7544\n",
            "Epoch 8/60\n",
            "40000/40000 [==============================] - 23s 584us/step - loss: 0.9303 - acc: 0.7218 - val_loss: 0.8246 - val_acc: 0.7578\n",
            "Epoch 9/60\n",
            "40000/40000 [==============================] - 23s 587us/step - loss: 0.8883 - acc: 0.7394 - val_loss: 0.8415 - val_acc: 0.7633\n",
            "Epoch 10/60\n",
            "40000/40000 [==============================] - 23s 587us/step - loss: 0.8595 - acc: 0.7531 - val_loss: 0.8310 - val_acc: 0.7743\n",
            "Epoch 11/60\n",
            "40000/40000 [==============================] - 24s 592us/step - loss: 0.8273 - acc: 0.7675 - val_loss: 0.8042 - val_acc: 0.7806\n",
            "Epoch 12/60\n",
            "40000/40000 [==============================] - 24s 589us/step - loss: 0.8066 - acc: 0.7770 - val_loss: 0.7585 - val_acc: 0.7995\n",
            "Epoch 13/60\n",
            "40000/40000 [==============================] - 24s 590us/step - loss: 0.7820 - acc: 0.7854 - val_loss: 0.7933 - val_acc: 0.7932\n",
            "Epoch 14/60\n",
            "40000/40000 [==============================] - 24s 588us/step - loss: 0.7668 - acc: 0.7942 - val_loss: 0.7382 - val_acc: 0.8123\n",
            "Epoch 15/60\n",
            "40000/40000 [==============================] - 24s 595us/step - loss: 0.7464 - acc: 0.8015 - val_loss: 0.7701 - val_acc: 0.8069\n",
            "Epoch 16/60\n",
            "40000/40000 [==============================] - 24s 594us/step - loss: 0.7297 - acc: 0.8085 - val_loss: 0.7809 - val_acc: 0.8034\n",
            "Epoch 17/60\n",
            "40000/40000 [==============================] - 24s 594us/step - loss: 0.7141 - acc: 0.8174 - val_loss: 0.8572 - val_acc: 0.7896\n",
            "Epoch 18/60\n",
            "40000/40000 [==============================] - 24s 594us/step - loss: 0.6994 - acc: 0.8222 - val_loss: 0.8008 - val_acc: 0.8040\n",
            "Epoch 19/60\n",
            "40000/40000 [==============================] - 25s 615us/step - loss: 0.6886 - acc: 0.8289 - val_loss: 0.8191 - val_acc: 0.7954\n",
            "Epoch 20/60\n",
            "40000/40000 [==============================] - 24s 599us/step - loss: 0.6804 - acc: 0.8306 - val_loss: 0.6987 - val_acc: 0.8298\n",
            "Epoch 21/60\n",
            "40000/40000 [==============================] - 27s 666us/step - loss: 0.6672 - acc: 0.8381 - val_loss: 0.7275 - val_acc: 0.8215\n",
            "Epoch 22/60\n",
            "40000/40000 [==============================] - 24s 602us/step - loss: 0.6589 - acc: 0.8388 - val_loss: 0.7263 - val_acc: 0.8277\n",
            "Epoch 23/60\n",
            "40000/40000 [==============================] - 25s 635us/step - loss: 0.6491 - acc: 0.8453 - val_loss: 0.7122 - val_acc: 0.8282\n",
            "Epoch 24/60\n",
            "40000/40000 [==============================] - 25s 620us/step - loss: 0.6384 - acc: 0.8462 - val_loss: 0.7473 - val_acc: 0.8241\n",
            "Epoch 25/60\n",
            "40000/40000 [==============================] - 24s 611us/step - loss: 0.6289 - acc: 0.8527 - val_loss: 0.7820 - val_acc: 0.8163\n",
            "Epoch 26/60\n",
            "40000/40000 [==============================] - 24s 610us/step - loss: 0.6177 - acc: 0.8559 - val_loss: 0.6998 - val_acc: 0.8409\n",
            "Epoch 27/60\n",
            "40000/40000 [==============================] - 24s 612us/step - loss: 0.6241 - acc: 0.8537 - val_loss: 0.6939 - val_acc: 0.8380\n",
            "Epoch 28/60\n",
            "40000/40000 [==============================] - 24s 610us/step - loss: 0.6180 - acc: 0.8584 - val_loss: 0.6808 - val_acc: 0.8456\n",
            "Epoch 29/60\n",
            "40000/40000 [==============================] - 25s 613us/step - loss: 0.6066 - acc: 0.8623 - val_loss: 0.6853 - val_acc: 0.8461\n",
            "Epoch 30/60\n",
            "40000/40000 [==============================] - 25s 613us/step - loss: 0.6011 - acc: 0.8651 - val_loss: 0.7224 - val_acc: 0.8357\n",
            "Epoch 31/60\n",
            "40000/40000 [==============================] - 25s 622us/step - loss: 0.6001 - acc: 0.8647 - val_loss: 0.6884 - val_acc: 0.8457\n",
            "Epoch 32/60\n",
            "40000/40000 [==============================] - 24s 590us/step - loss: 0.5952 - acc: 0.8672 - val_loss: 0.7635 - val_acc: 0.8321\n",
            "Epoch 33/60\n",
            "40000/40000 [==============================] - 23s 585us/step - loss: 0.5838 - acc: 0.8728 - val_loss: 0.7266 - val_acc: 0.8406\n",
            "Epoch 34/60\n",
            "40000/40000 [==============================] - 23s 586us/step - loss: 0.5848 - acc: 0.8703 - val_loss: 0.6932 - val_acc: 0.8440\n",
            "Epoch 35/60\n",
            "40000/40000 [==============================] - 23s 585us/step - loss: 0.5779 - acc: 0.8762 - val_loss: 0.6873 - val_acc: 0.8527\n",
            "Epoch 36/60\n",
            "40000/40000 [==============================] - 23s 584us/step - loss: 0.5842 - acc: 0.8751 - val_loss: 0.7151 - val_acc: 0.8442\n",
            "Epoch 37/60\n",
            "40000/40000 [==============================] - 23s 585us/step - loss: 0.5698 - acc: 0.8792 - val_loss: 0.7289 - val_acc: 0.8455\n",
            "Epoch 38/60\n",
            "40000/40000 [==============================] - 24s 589us/step - loss: 0.5717 - acc: 0.8789 - val_loss: 0.6920 - val_acc: 0.8544\n",
            "Epoch 39/60\n",
            "40000/40000 [==============================] - 24s 591us/step - loss: 0.5695 - acc: 0.8781 - val_loss: 0.7055 - val_acc: 0.8476\n",
            "Epoch 40/60\n",
            "40000/40000 [==============================] - 24s 592us/step - loss: 0.5625 - acc: 0.8814 - val_loss: 0.7186 - val_acc: 0.8424\n",
            "Epoch 41/60\n",
            "40000/40000 [==============================] - 24s 592us/step - loss: 0.5501 - acc: 0.8866 - val_loss: 0.7341 - val_acc: 0.8428\n",
            "Epoch 42/60\n",
            "40000/40000 [==============================] - 24s 592us/step - loss: 0.5547 - acc: 0.8847 - val_loss: 0.6940 - val_acc: 0.8479\n",
            "Epoch 43/60\n",
            "40000/40000 [==============================] - 24s 589us/step - loss: 0.5485 - acc: 0.8875 - val_loss: 0.7327 - val_acc: 0.8475\n",
            "Epoch 44/60\n",
            "40000/40000 [==============================] - 24s 607us/step - loss: 0.5484 - acc: 0.8857 - val_loss: 0.7309 - val_acc: 0.8449\n",
            "Epoch 45/60\n",
            "40000/40000 [==============================] - 24s 595us/step - loss: 0.5483 - acc: 0.8880 - val_loss: 0.7283 - val_acc: 0.8461\n",
            "Epoch 46/60\n",
            "40000/40000 [==============================] - 24s 595us/step - loss: 0.5463 - acc: 0.8894 - val_loss: 0.6947 - val_acc: 0.8582\n",
            "Epoch 47/60\n",
            "40000/40000 [==============================] - 24s 595us/step - loss: 0.5429 - acc: 0.8903 - val_loss: 0.7128 - val_acc: 0.8520\n",
            "Epoch 48/60\n",
            "40000/40000 [==============================] - 26s 646us/step - loss: 0.5428 - acc: 0.8901 - val_loss: 0.7216 - val_acc: 0.8487\n",
            "Epoch 49/60\n",
            "40000/40000 [==============================] - 24s 611us/step - loss: 0.5446 - acc: 0.8898 - val_loss: 0.7272 - val_acc: 0.8509\n",
            "Epoch 50/60\n",
            "40000/40000 [==============================] - 25s 625us/step - loss: 0.5444 - acc: 0.8918 - val_loss: 0.7258 - val_acc: 0.8504\n",
            "Epoch 51/60\n",
            "40000/40000 [==============================] - 25s 634us/step - loss: 0.5333 - acc: 0.8952 - val_loss: 0.6885 - val_acc: 0.8561\n",
            "Epoch 52/60\n",
            "40000/40000 [==============================] - 24s 611us/step - loss: 0.5311 - acc: 0.8946 - val_loss: 0.7278 - val_acc: 0.8551\n",
            "Epoch 53/60\n",
            "40000/40000 [==============================] - 24s 612us/step - loss: 0.5315 - acc: 0.8947 - val_loss: 0.6998 - val_acc: 0.8556\n",
            "Epoch 54/60\n",
            "40000/40000 [==============================] - 24s 612us/step - loss: 0.5245 - acc: 0.8972 - val_loss: 0.7258 - val_acc: 0.8511\n",
            "Epoch 55/60\n",
            "40000/40000 [==============================] - 25s 636us/step - loss: 0.5313 - acc: 0.8957 - val_loss: 0.7274 - val_acc: 0.8527\n",
            "Epoch 56/60\n",
            "40000/40000 [==============================] - 25s 613us/step - loss: 0.5302 - acc: 0.8951 - val_loss: 0.7367 - val_acc: 0.8479\n",
            "Epoch 57/60\n",
            "40000/40000 [==============================] - 24s 611us/step - loss: 0.5222 - acc: 0.8990 - val_loss: 0.7055 - val_acc: 0.8591\n",
            "Epoch 58/60\n",
            "40000/40000 [==============================] - 25s 628us/step - loss: 0.5185 - acc: 0.9010 - val_loss: 0.7463 - val_acc: 0.8477\n",
            "Epoch 59/60\n",
            "40000/40000 [==============================] - 24s 608us/step - loss: 0.5249 - acc: 0.8997 - val_loss: 0.7723 - val_acc: 0.8378\n",
            "Epoch 60/60\n",
            "40000/40000 [==============================] - 23s 587us/step - loss: 0.5181 - acc: 0.9010 - val_loss: 0.7092 - val_acc: 0.8573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S2NfzPoCo12s",
        "colab_type": "code",
        "outputId": "5054f25c-96e4-4af9-dfa7-c13c07d350b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        }
      },
      "cell_type": "code",
      "source": [
        "#print(epochs)\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# training and validation accuracy\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='validation acc')\n",
        "plt.title('training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# training and validation loss\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='validation loss')\n",
        "plt.title('training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFZCAYAAACv05cWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFZCAYAAACv05cWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuIJM6bIJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "pOki6JwFjtIY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Model Performance on Test data using simple hold validation"
      ]
    },
    {
      "metadata": {
        "id": "kLX_ovaNj0ps",
        "colab_type": "code",
        "outputId": "64a96677-004c-459f-923c-a4d5c593c4bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "result = model.evaluate(test_images_norm, test_labels_norm, verbose=1)\n",
        "print('Test accuracy:', result[1])\n",
        "print('Test loss:', result[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 3s 254us/step\n",
            "Test accuracy: 0.8533\n",
            "Test loss: 0.7242817924499512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cEU96zN_gSMY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###K-fold validation "
      ]
    },
    {
      "metadata": {
        "id": "eenjVdBBRP7J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eb6b17f1-8027-4ad9-e13d-4de175995315"
      },
      "cell_type": "code",
      "source": [
        "print(train_images_norm.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "39OAAf39gkHn",
        "colab_type": "code",
        "outputId": "23fe9df3-c502-4ed1-ae2f-5479d486d155",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1057
        }
      },
      "cell_type": "code",
      "source": [
        "#train_images_norm_k = (train_images/255).astype('float32')\n",
        "kfold_results = []\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=None, shuffle=False)\n",
        "\n",
        "for train_idx_k, val_idx_k in kf.split(train_images_norm):\n",
        "  train_images_norm_k, val_images_norm_k = train_images_norm[train_idx_k], train_images_norm[val_idx_k]\n",
        "  train_labels_norm_k, val_labels_norm_k = train_labels_norm[train_idx_k], train_labels_norm[val_idx_k]\n",
        "  \n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "  epochs = 5\n",
        "  history = model.fit(train_images_norm_k, train_labels_norm_k, epochs=epochs, batch_size=40, validation_data=(val_images_norm_k, val_labels_norm_k))\n",
        "  \n",
        "  result = model.evaluate(test_images_norm, test_labels_norm)\n",
        "  kfold_results.append(result[1]*100)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 32000 samples, validate on 8000 samples\n",
            "Epoch 1/5\n",
            "32000/32000 [==============================] - 37s 1ms/step - loss: 0.5506 - acc: 0.8972 - val_loss: 0.5698 - val_acc: 0.8919\n",
            "Epoch 2/5\n",
            "32000/32000 [==============================] - 33s 1ms/step - loss: 0.5542 - acc: 0.8987 - val_loss: 0.5912 - val_acc: 0.8874\n",
            "Epoch 3/5\n",
            "32000/32000 [==============================] - 31s 968us/step - loss: 0.5545 - acc: 0.8975 - val_loss: 0.6175 - val_acc: 0.8789\n",
            "Epoch 4/5\n",
            "32000/32000 [==============================] - 29s 914us/step - loss: 0.5517 - acc: 0.8991 - val_loss: 0.5861 - val_acc: 0.8894\n",
            "Epoch 5/5\n",
            "32000/32000 [==============================] - 29s 919us/step - loss: 0.5610 - acc: 0.8975 - val_loss: 0.6265 - val_acc: 0.8807\n",
            "10000/10000 [==============================] - 2s 240us/step\n",
            "Train on 32000 samples, validate on 8000 samples\n",
            "Epoch 1/5\n",
            "32000/32000 [==============================] - 32s 1ms/step - loss: 0.6111 - acc: 0.8840 - val_loss: 0.4247 - val_acc: 0.9448\n",
            "Epoch 2/5\n",
            "32000/32000 [==============================] - 31s 967us/step - loss: 0.5984 - acc: 0.8854 - val_loss: 0.4237 - val_acc: 0.9453\n",
            "Epoch 3/5\n",
            "32000/32000 [==============================] - 29s 917us/step - loss: 0.5857 - acc: 0.8874 - val_loss: 0.4320 - val_acc: 0.9384\n",
            "Epoch 4/5\n",
            "32000/32000 [==============================] - 31s 958us/step - loss: 0.5758 - acc: 0.8932 - val_loss: 0.4684 - val_acc: 0.9312\n",
            "Epoch 5/5\n",
            "32000/32000 [==============================] - 30s 948us/step - loss: 0.5712 - acc: 0.8947 - val_loss: 0.5161 - val_acc: 0.9107\n",
            "10000/10000 [==============================] - 3s 254us/step\n",
            "Train on 32000 samples, validate on 8000 samples\n",
            "Epoch 1/5\n",
            "32000/32000 [==============================] - 34s 1ms/step - loss: 0.6052 - acc: 0.8835 - val_loss: 0.4903 - val_acc: 0.9130\n",
            "Epoch 2/5\n",
            "32000/32000 [==============================] - 31s 954us/step - loss: 0.5845 - acc: 0.8904 - val_loss: 0.4309 - val_acc: 0.9415\n",
            "Epoch 3/5\n",
            "32000/32000 [==============================] - 31s 957us/step - loss: 0.5816 - acc: 0.8887 - val_loss: 0.4601 - val_acc: 0.9319\n",
            "Epoch 4/5\n",
            "32000/32000 [==============================] - 32s 988us/step - loss: 0.5808 - acc: 0.8913 - val_loss: 0.4391 - val_acc: 0.9374\n",
            "Epoch 5/5\n",
            "32000/32000 [==============================] - 31s 961us/step - loss: 0.5687 - acc: 0.8977 - val_loss: 0.5239 - val_acc: 0.9088\n",
            "10000/10000 [==============================] - 3s 282us/step\n",
            "Train on 32000 samples, validate on 8000 samples\n",
            "Epoch 1/5\n",
            "32000/32000 [==============================] - 36s 1ms/step - loss: 0.5945 - acc: 0.8871 - val_loss: 0.4336 - val_acc: 0.9421\n",
            "Epoch 2/5\n",
            "32000/32000 [==============================] - 33s 1ms/step - loss: 0.5792 - acc: 0.8920 - val_loss: 0.4214 - val_acc: 0.9433\n",
            "Epoch 3/5\n",
            "32000/32000 [==============================] - 33s 1ms/step - loss: 0.5714 - acc: 0.8960 - val_loss: 0.4557 - val_acc: 0.9326\n",
            "Epoch 4/5\n",
            "32000/32000 [==============================] - 32s 1ms/step - loss: 0.5718 - acc: 0.8956 - val_loss: 0.4685 - val_acc: 0.9289\n",
            "Epoch 5/5\n",
            "32000/32000 [==============================] - 33s 1ms/step - loss: 0.5687 - acc: 0.8964 - val_loss: 0.4900 - val_acc: 0.9205\n",
            "10000/10000 [==============================] - 3s 275us/step\n",
            "Train on 32000 samples, validate on 8000 samples\n",
            "Epoch 1/5\n",
            "32000/32000 [==============================] - 39s 1ms/step - loss: 0.5937 - acc: 0.8893 - val_loss: 0.4219 - val_acc: 0.9465\n",
            "Epoch 2/5\n",
            "32000/32000 [==============================] - 34s 1ms/step - loss: 0.5796 - acc: 0.8938 - val_loss: 0.4816 - val_acc: 0.9251\n",
            "Epoch 3/5\n",
            "32000/32000 [==============================] - 34s 1ms/step - loss: 0.5831 - acc: 0.8917 - val_loss: 0.4515 - val_acc: 0.9367\n",
            "Epoch 4/5\n",
            "32000/32000 [==============================] - 33s 1ms/step - loss: 0.5625 - acc: 0.8970 - val_loss: 0.4439 - val_acc: 0.9365\n",
            "Epoch 5/5\n",
            "32000/32000 [==============================] - 31s 974us/step - loss: 0.5699 - acc: 0.8957 - val_loss: 0.5433 - val_acc: 0.9061\n",
            "10000/10000 [==============================] - 3s 263us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bE2qG1Q3XRpC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ddc31273-2c70-4538-b2d8-e519e0083848"
      },
      "cell_type": "code",
      "source": [
        "kfold_results"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[84.2, 83.85000000000001, 83.19, 84.58, 82.80999999999999]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}